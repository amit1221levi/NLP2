{"cells":[{"cell_type":"markdown","metadata":{"id":"P0Iyg6btLW9M"},"source":["#  Assignment 2 - Transfer Learning and Data Augmentation 💬\n","\n","Welcome to the **second assignment** for the **CS-552: Modern NLP course**!\n","\n","> - 😀 Name: Amit LEvi\n","> - ✉️ Email: amit.levi@epfl.ch\n","> - 🪪 SCIPER: **XXXXXX**"]},{"cell_type":"markdown","metadata":{"id":"_XjnQhbFIJUu"},"source":["<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;\">\n","\n","## **Assignment Description**\n","- In the first part of this assignment, you will need to implement training (fine-tuning) and evaluation of a pre-trained language model ([DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) ), on natural language inference (NLI) task for recognizing textual entailment (RTE).\n","\n","- Following the first finetuning task, you will need to identify the shortcut (i.e. some salient or toxic features) that the model learnt for the specific task. \n","\n","- For part-3, you are supposed to annotate 100 randomly assigned test datapoints as ground-truth labels. Additionally, the cross annotation should be conducted by another one or two annotators, and you will learn about how to calculate the agreement statistics as a significant characteristic reflecting the quality of a collected dataset.\n","\n","- For part-4, since the human annotation is quite time- and effort-consuming, there are plenty of ways to get silver-labels from automatic labeling to augment the dataset scale. We provide the reference to some simple methods (EDA and Back Translation) but you are encouraged to explore other advanced mechanisms. You will evaluate the improvement of your model performance by using your data augmentation method.\n","\n","For each part, you will need to complete the code in the corresponding `.py` files (`nli.py` for Part-1, `shortcut.py` for Part-2, `eda.py` for Part-4). You will be provided with the function descriptions and detailed instructions about the code snippet you need to write.\n","\n","\n","### Table of Contents\n","- **[PART 1: Model Finetuning for NLI](#1)**\n","    - [1.1 Data Processing](#11)\n","    - [1.2 Model Training and Evaluation](#12)\n","- **[PART 2: Identify Model Shortcut](#2)**\n","    - [2.1 Word-Pair Pattern Extraction](#21)\n","    - [2.2 Distill Potentially Useful Patterns](#22)\n","    - [2.3 Case Study](#23)\n","- **[PART 3: Annotate New Data](#3)**\n","    - [3.1 Write an Annotation Guideline](#31)\n","    - [3.2 Annotate Your 100 Datapoints with Partner(s)](#32)\n","    - [3.3 Agreement Measure](#33)\n","    - [3.4 Robustness Check](#34)\n","- **[PART 4: Data Augmentation](#4)**\n","    \n","### Deliverables\n","\n","- ✅ This jupyter notebook\n","- ✅ `nli.py` file\n","- ✅ `shortcut.py` file\n","- ✅ Finetuned DistilBERT models for NLI task (Part 1 and Part 4)\n","- ✅ Annotated and cross-annotated data files (Part 3)\n","- ✅ New dataset from data augmentation (Part 4)\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"lluaZwaS-0v9"},"source":["### Google Colab Setup\n","If you are using Google Colab notebook for this assignment, you will need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. Follow the popped window, sign in to your Google account. (The same account you used to store this notebook!)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24399,"status":"ok","timestamp":1681409449255,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"},"user_tz":-120},"id":"VfVHqiSvK1aB","outputId":"73832e05-5c61-40e3-ae19-5c892704ef30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"opCteyIv-_kS"},"source":["Now first click the 4th left-side bar (named Files), then click the 2nd bar popped under Files column (named Refresh), under \"/drive/MyDrive/\" find the Assignment 2 folder that you uploaded to your Google Drive, copy its path and fill it in below. If everything is working correctly, then running the folowing cell should print the filenames from the assignment:\n","\n","```\n","['Assignment2.ipynb', 'requirements.txt', 'runs', 'predictions', 'nli_data', 'testA2.py', 'nli.py', 'shortcut.py']\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1681409449256,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"},"user_tz":-120},"id":"agEgK0kdrUdT","outputId":"67dc72f7-8481-4e77-e557-b8cc2cbebbb0"},"outputs":[{"output_type":"stream","name":"stdout","text":["['README.md', 'Assignment2.ipynb', 'nli_data', 'requirements.txt', '__pycache__', 'testA2.py', 'runs', 'shortcut.py', 'predictions', 'eda.py', 'nli.py', '45_labeled (1).jsonl', '.ipynb_checkpoints', 'val_data.jsonl', 'student1_test.jsonl', 'student2_test.jsonl']\n"]}],"source":["import os\n","# TODO: Fill in the path where you download the Assignment folder into\n","ROOT_PATH = \"/content/drive/MyDrive/a2-amit1221levi-main/A2\"# Replace with your directory to A2 folder\n","print(os.listdir(ROOT_PATH))"]},{"cell_type":"markdown","metadata":{"id":"_5mABwvHy5-e"},"source":["Before we start, we also need to run some boilerplate code to set up our environment, same as previous assignments. You'll need to rerun this setup code each time you start the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZhJT7Fo4_D1f"},"outputs":[],"source":["requirements = ROOT_PATH + \"/requirements.txt\"\n","!pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116\n","\n","!pip install -r {requirements}"]},{"cell_type":"markdown","metadata":{"id":"RUw9ycDa21dl"},"source":["\n","Run this cell to load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PVAoLPQ_I7c"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mA1Qk-_K_LRm"},"outputs":[],"source":["from copy import deepcopy\n","import numpy as np \n","from tqdm import tqdm\n","import jsonlines\n","import sys\n","import time\n","import random\n","\n","import torch\n","import torch.utils.data\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW, get_constant_schedule_with_warmup\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification"]},{"cell_type":"markdown","metadata":{"id":"csa48DhDr0td"},"source":["Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n","\n","```\n","Hello A2!\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U10S-b9BrNxj","executionInfo":{"status":"ok","timestamp":1681409623098,"user_tz":-120,"elapsed":1990,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"outputId":"e02a7a76-83d5-419d-a8f1-2665236cbe0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hello A2!\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["sys.path.append(ROOT_PATH)\n","\n","from testA2 import hello_A2\n","hello_A2()"]},{"cell_type":"markdown","metadata":{"id":"dnWpzntscWUE"},"source":["Note that if CUDA is not enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rqb9cwkNIEHr","executionInfo":{"status":"ok","timestamp":1681409623099,"user_tz":-120,"elapsed":6,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"outputId":"6d24d587-618f-4e11-c133-45b46d4dbf03"},"outputs":[{"output_type":"stream","name":"stdout","text":["Good to go!\n"]}],"source":["if torch.cuda.is_available():\n","  print('Good to go!')\n","else:\n","  print('Please set GPU via Edit -> Notebook Settings.')"]},{"cell_type":"markdown","metadata":{"id":"9MSpYuMcyHfl"},"source":["### Local Setup\n","If you skip Google Colab setup, you still need to fill in the path where you download the Assignment folder, and install required packages."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQHs7vHhydij"},"outputs":[],"source":["#ROOT_PATH = \"MyDrive/A2\" # Replace with your directory to A2 folder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKeGSDpAyigE"},"outputs":[],"source":["#requirements = ROOT_PATH + \"/requirements.txt\"\n","#!pip install -r {requirements}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6b-Enimyywz"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6dgLZ9kyqpO"},"outputs":[],"source":["from copy import deepcopy\n","import numpy as np \n","from tqdm import tqdm\n","import jsonlines\n","import sys\n","import time, os\n","import random\n","\n","import torch\n","import torch.utils.data\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW, get_constant_schedule_with_warmup\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification"]},{"cell_type":"markdown","metadata":{"id":"rHhgkhaH-IUl"},"source":["<a name=\"1\"></a>\n","## **PART 1: Finetuning DistilBERT for NLI**\n","---"]},{"cell_type":"markdown","metadata":{"id":"tD2YPuqeIYBN"},"source":["### **What is the NLI task?🧐**\n","> Given a pair of sentences, denoted as a \"premise\" sentence and a \"hypothesis\" sentence, NLI (or RTE) aims to determine their logical relationship, i.e. whether they are logically follow (entailment), unfollow (contradiction) or are undetermined (neutral) to each other.\n","\n","> Defined as a machine learning task, NLI can be considered as a 3-classes (entailment, contradiction, or neutral) classification task, with a sentence-pair input (\"hypothesis\" and “premise”).\n","\n","> **You can run the following cell to have the first glance at your data**. Each data sample is a python dictionary, which consists of following components:\n","- premise sentence (*'premise'*), \n","- hypothesis sentence (*'hypothesis'*) \n","- domain (*'domain'*): describing the topic of premise and hypothesis sentences (e.g., government regulations, telephone talks, etc.)\n","- label (*'label'*): indicating the logical relation between premise and hypothesis (i.e., entailment, contradiction, or neutral)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-ODgcNUqYtm","executionInfo":{"status":"ok","timestamp":1681409625510,"user_tz":-120,"elapsed":1022,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"outputId":"5314eeac-280a-4576-b426-ca50db4e05ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'premise': 'The new rights are nice enough', 'hypothesis': 'Everyone really likes the newest benefits ', 'domain': 'slate', 'label': 'neutral'}\n","{'premise': 'This site includes a list of all award winners and a searchable database of Government Executive articles.', 'hypothesis': 'The Government Executive articles housed on the website are not able to be searched.', 'domain': 'government', 'label': 'contradiction'}\n","{'premise': \"uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him\", 'hypothesis': 'I like him for the most part, but would still enjoy seeing someone beat him.', 'domain': 'telephone', 'label': 'entailment'}\n"]}],"source":["# If you use Google Colab, then data_dir = 'GOOGLE_DRIVE_PATH/nli_data'\n","data_dir = ROOT_PATH+'/nli_data'\n","data_dev_path = os.path.join(data_dir, 'dev_in_domain.jsonl')\n","with jsonlines.open(data_dev_path, \"r\") as reader:\n","    for sid, sample in enumerate(reader.iter()):\n","        print(sample)\n","        if sid == 2:\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FF-dRWc7MZlL"},"outputs":[],"source":["# Enter enter your Sciper number\n","SCIPER = '366804'\n","seed = int(SCIPER)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9UPdzLSi4ZVt","executionInfo":{"status":"ok","timestamp":1681414358767,"user_tz":-120,"elapsed":3,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"outputId":"71a110be-2b25-4552-986d-81429fb3b9c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Your random seed is:  366804\n"]}],"source":["print('Your random seed is: ', seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":366,"referenced_widgets":["9a1df9d4f3f0428b9906c1693850f9f7","2ad8894fb435407e8ed3d5da42ba7deb","2348f8de400d42c186af9119667af37a","8a0b6b3d31a6496e9cf38f75f3889464","58e0d63d3b3d4d57b097eb6c05e03a16","55cd71bb62c84ea8b8aebe401954fa20","40002dece1344ecdb1221667c4d0d8ec","b496467e359f45cb91f5e23c090fb7d2","75afd907000c4fa49150f84a66653907","22ded636316d415bb9c3451ed31d3590","c1b611bcfc1f49ef866f18b01fc49f00","73874600d27d47d49caa3cbecacd4010","4fd27a3cfb894ffe9110435d2923e7e5","da361491076146278f0e9673fd2e5808","8cbc37bae1b1489db43729a5af78005c","28e18da587ca4476ae2775df921edf2f","c45b96c5790a4bd08e26634d34b00e02","c159b26298c04ee39787e9aaea19c9b7","8e63ea082f54476e8d0d202e3a328b6c","aed796e68e1b4405b93496f870a508e5","d707f5b2514d452cb642521079dd8ecb","e6bbfc5a675444acb0b832da5fc63408","642526fbd4d64eb298c9963dd07173c1","42678b05e089459b93b05d05c1dcfa92","62ecbcd2cf6f4742a1c45608b8030cf4","7336d3a9622f486db40ab9c91107b4b6","6ae67d0f0ddf4b37aadd56145857ce74","dc65c6e1803a4240bc0118f6077ec321","0d33de5bed534625b404f109256a493a","b8badace30e2431885dead135ce97c3c","590820d5ac124e5b9c79da8b459e9e6e","d41313cebcc0486abab86bb3d321e02f","98ade5be0fca4d6ebbde6e743b19fb83","7b6211eb7cf84ae5bcae10ab9c52c6c2","37eb2764460b483da8490a576da76aaf","b556def56f8646e2a9d06c25205bd815","fa8173b019dd4eecab6fccf513946165","7971804787e143a68588b01df694b8db","e2cd64e8ad0847b890dd425d3e5d13bb","2fe28310e7ba40609f9292c30528c9a1","969c23c4fe854cb6ae57fe36b364ac51","a074ddca842b4f3c8ae5001d019b793e","884645a3b2e3458eb770794c7cf1d6f9","8040403ccec94434beae96b4299b9b0d"]},"id":"GnX8VC4C0sHW","executionInfo":{"status":"ok","timestamp":1681409632745,"user_tz":-120,"elapsed":7243,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"outputId":"57651c82-64cf-4b2c-9b84-f841271a5317"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a1df9d4f3f0428b9906c1693850f9f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73874600d27d47d49caa3cbecacd4010"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"642526fbd4d64eb298c9963dd07173c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b6211eb7cf84ae5bcae10ab9c52c6c2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# We use the following pretrained tokenizer and model\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)"]},{"cell_type":"markdown","metadata":{"id":"fCETOFT2dB4u"},"source":["### **1.1 Dataset Processing**\n","Our first step is to load datasets for NLI task by constructing a Pytorch Dataset. Specifically, we will need to implement tokenization and padding with a HuggingFace pre-trained tokenizer.\n","\n","**Complete `NLIDataset` class following the instructions in `nli.py`, and test by running the following cell.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_5Sya9W5BTDl","executionInfo":{"status":"ok","timestamp":1681409908861,"user_tz":-120,"elapsed":16249,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"outputId":"2fea93ba-da70-4417-edb2-d13f763dd4c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["9815it [00:15, 629.21it/s]"]},{"output_type":"stream","name":"stdout","text":["NLIDataset test correct ✅\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["from nli import NLIDataset\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","dataset = NLIDataset(ROOT_PATH+\"/nli_data/dev_in_domain.jsonl\", tokenizer)\n","\n","from testA2 import test_NLIDataset\n","test_NLIDataset(dataset)"]},{"cell_type":"markdown","metadata":{"id":"W0weQpG6_3vO"},"source":["### **1.2 Model Training and Evaluation**\n","Next, we will implement the training and evaluation process to finetune the model. For model training, you will need to calculate the loss and update the model weights by update the optimizer. Additionally, we add a learning rate schedular to adopt an adaptive learning rate during the whole training process. \n","\n","For evaluation, you will need to compute accuracy and F1 scores to assess the model performance. \n","\n","**Complete the `compute_metric()`, `train()` and `evaluate()` functions following the instructions in the `nli.py` file, you can test compute_metric() by running the following cell.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6w7Leraw4tIY","executionInfo":{"status":"ok","timestamp":1681409908861,"user_tz":-120,"elapsed":5,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"outputId":"dde8c12f-775d-498d-eea7-3e980bca5ee1"},"outputs":[{"output_type":"stream","name":"stdout","text":["compute_metric test correct ✅\n"]}],"source":["from nli import compute_metrics, train, evaluate\n","\n","from testA2 import test_compute_metrics\n","test_compute_metrics(compute_metrics)"]},{"cell_type":"markdown","metadata":{"id":"pvCUS748_3vS"},"source":["#### **Start Training and Validation!**\n","\n","Try the following different hyperparameter settings, compare and discuss the results. (Other hyperparameters should not be changed.)\n","\n","> A. learning_rate 2e-5\n","\n","> B. learning_rate 5e-5\n","\n","**Note:** *Each training will take about 1 hour using a GPU, please keep your computer and notebook active during the training.*\n","\n","**Questions: Which learning rate is better? Explain your answers.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zn66mMOj_3vS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681403707205,"user_tz":-120,"elapsed":121367,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"outputId":"b5441be5-6dbe-4b41-9b3b-bbe45c268b94"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["98176it [01:40, 976.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["9815it [00:11, 866.77it/s] \n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","model.to(device)\n","\n","train_dataset = NLIDataset(ROOT_PATH+\"/nli_data/train.jsonl\", tokenizer)\n","dev_dataset = NLIDataset(ROOT_PATH+\"/nli_data/dev_in_domain.jsonl\", tokenizer)\n","\n","batch_size = 16\n","epochs = 4\n","max_grad_norm = 1.0\n","warmup_percent = 0.3\n","model_save_root = ROOT_PATH+'/runs/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"an7lzsAywGxE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e19f9b41-bb15-4408-9428-038e40c59958","executionInfo":{"status":"ok","timestamp":1681336414606,"user_tz":-120,"elapsed":3128857,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 6136/6136 [12:50<00:00,  7.97it/s]\n","Evaluation: 100%|██████████| 614/614 [00:23<00:00, 25.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Training Loss: 1.251 | Validation Loss: 1.103\n","Epoch 0 NLI Validation:\n","Accuracy: 68.74% | F1: (63.93%, 72.18%, 57.06%) | Macro-F1: 16.44%\n","Model Saved!\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 6136/6136 [12:37<00:00,  8.10it/s]\n","Evaluation: 100%|██████████| 614/614 [00:23<00:00, 26.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Training Loss: 1.107 | Validation Loss: 1.099\n","Epoch 1 NLI Validation:\n","Accuracy: 74.44% | F1: (69.23%, 78.16%, 61.78%) | Macro-F1: 17.45%\n","Model Saved!\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 6136/6136 [12:32<00:00,  8.16it/s]\n","Evaluation: 100%|██████████| 614/614 [00:23<00:00, 26.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 2 | Training Loss: 3.447 | Validation Loss: 1.126\n","Epoch 2 NLI Validation:\n","Accuracy: 66.82% | F1: (62.14%, 70.16%, 55.46%) | Macro-F1: 16.09%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 6136/6136 [12:32<00:00,  8.16it/s]\n","Evaluation: 100%|██████████| 614/614 [00:22<00:00, 26.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 3 | Training Loss: 1.107 | Validation Loss: 1.111\n","Epoch 3 NLI Validation:\n","Accuracy: 74.44% | F1: (69.23%, 78.16%, 61.78%) | Macro-F1: 17.45%\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","learning_rate = 0.1 # play around with this hyperparameter\n","\n","train(train_dataset, dev_dataset, model, device, batch_size, epochs,\n","      learning_rate, warmup_percent, max_grad_norm, model_save_root)"]},{"cell_type":"markdown","metadata":{"id":"wGuzGJCB_3vT"},"source":["### **Fine-Grained Validation**\n","\n","Use the model checkpoint saved under the first hyperparameter setting (learning_rate 2e-5) in 1.4, check the model performance on each domain subsets of the validation set, report the validation loss, accuracy, F1 scores and Macro-F1 on each domain, compare and discuss the results.\n","\n","**Questions: On which domain does the model perform the best? the worst? Give some possible explanations of why the model's best-performed domain is easier, and why the model's worst-performed domain is more challenging. Use some examples to support your explanations.**\n","\n","**Note:** To find examples for supporting your discussion, save the model prediction results on each domain under the './predictions/' folder, by specifying the *result_save_file* of the *evaluate* function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCWWJjTP_3vT","executionInfo":{"status":"ok","timestamp":1681337464658,"user_tz":-120,"elapsed":1053,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"65037b7b-e638-45ff-b0da-920ede7a99b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:00, 321.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:00, 1215.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:00, 342.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:00, 1274.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:00, 308.22it/s]\n"]}],"source":["batch_size = 16\n","learning_rate = 2e-5\n","warmup_percent = 0.3\n","checkpoint = ROOT_PATH+'/runs/lr{}-warmup{}'.format(learning_rate, warmup_percent)\n","\n","# Split the validation sets into subsets with different domains\n","# Save the subsets under './nli_data/'\n","# Replace \"...\" with your code\n","\n","# Split the validation sets into subsets with different domains\n","# Save the subsets under './nli_data/'\n","data_dir = ROOT_PATH + '/nli_data'\n","dev_path = os.path.join(data_dir, 'dev_in_domain.jsonl')\n","domains = ['travel', 'telephone', 'fiction', 'government', 'slate']\n","domain_paths = [os.path.join(data_dir, '{}.jsonl'.format(domain)) for domain in domains]\n","\n","# Create the domain-specific files and write the header\n","for domain_path in domain_paths:\n","    with jsonlines.open(domain_path, 'w') as writer:\n","        writer.write({'premise': 'premise', 'hypothesis': 'hypothesis', 'label': 'label', 'domain': domain_path})\n","\n","# Read the validation data and split it by domain\n","dev_data_dict = {}\n","with jsonlines.open(dev_path, 'r') as reader:\n","    for sample in reader:\n","        premise = sample['premise']\n","        hypothesis = sample['hypothesis']\n","        label = sample['label']\n","        domain = sample['domain']\n","        for i, domain_name in enumerate(domains):\n","            if domain_name in domain:\n","                domain_path = domain_paths[i]\n","                if domain_name not in dev_data_dict:\n","                    dev_dataset = NLIDataset(domain_path, DistilBertTokenizer.from_pretrained('distilbert-base-uncased'))\n","                    dev_data_dict[domain_name] = dev_dataset\n","                break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4J2pu60xHTd","executionInfo":{"status":"ok","timestamp":1681337500764,"user_tz":-120,"elapsed":36111,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a1e1f0df-4760-49ae-bb0b-017fe3d6189b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1973it [00:00, 1994.78it/s]\n","Evaluation: 100%|██████████| 124/124 [00:03<00:00, 34.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Domain: fiction\n","Validation Loss: 1.098 | Accuracy: 75.04%\n","F1: (69.79%, 78.79%, 62.28%) | Macro-F1: 24.11%\n","Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1945it [00:03, 599.33it/s]\n","Evaluation: 100%|██████████| 122/122 [00:04<00:00, 25.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Domain: government\n","Validation Loss: 1.097 | Accuracy: 75.25%\n","F1: (69.99%, 79.02%, 62.46%) | Macro-F1: 21.81%\n","Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1955it [00:01, 1380.37it/s]\n","Evaluation: 100%|██████████| 123/123 [00:04<00:00, 25.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Domain: slate\n","Validation Loss: 1.098 | Accuracy: 72.94%\n","F1: (67.83%, 76.58%, 60.54%) | Macro-F1: 23.50%\n","Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1966it [00:02, 811.61it/s]\n","Evaluation: 100%|██████████| 123/123 [00:05<00:00, 20.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Domain: telephone\n","Validation Loss: 1.100 | Accuracy: 74.77%\n","F1: (69.54%, 78.51%, 62.06%) | Macro-F1: 22.82%\n","Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1976it [00:02, 736.34it/s]\n","Evaluation: 100%|██████████| 124/124 [00:05<00:00, 24.22it/s]"]},{"output_type":"stream","name":"stdout","text":["Domain: travel\n","Validation Loss: 1.098 | Accuracy: 73.54%\n","F1: (68.39%, 77.22%, 61.04%) | Macro-F1: 31.79%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["\n","\n","\n","for domain in [\"fiction\", \"government\", \"slate\", \"telephone\", \"travel\"]:\n","    \n","    # Evaluate and save prediction results in each domain\n","    # Evaluate and save prediction results in each domain\n","    # Calculate the evaluation metrics\n","    # Load the dataset for the current domain\n","    dev_dataset = NLIDataset(os.path.join(data_dir, f\"dev_{domain}.jsonl\"), tokenizer)  \n","    # Evaluate and save prediction results in each domain\n","    result_save_file = os.path.join(ROOT_PATH, \"predictions\", f\"{domain}_predictions.jsonl\")\n","    dev_loss, acc, f1_ent, f1_neu, f1_con = evaluate(dev_dataset, model, device, batch_size, result_save_file=result_save_file)\n","    macro_f1 = (f1_ent + f1_neu + f1_con) / 3\n","    \n","    # Print the evaluation metrics\n","    print(f'Domain: {domain}')\n","    print(f'Validation Loss: {dev_loss:.3f} | Accuracy: {acc*100:.2f}%')\n","    print(f'F1: ({f1_ent*100:.2f}%, {f1_neu*100:.2f}%, {f1_con*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6NyMZ5E4-QxM"},"source":["## **Task2: Identify Shortcuts**\n","\n","We aim to find some shortcuts that the model in 1.4 (under the first hyperparameter setting) has learned."]},{"cell_type":"markdown","metadata":{"id":"4lCHLdaH_3vT"},"source":["### **2.1 Word-Pair Pattern Extraction**\n","\n","We consider to exatrct simple word-pair patterns that the model may have learned from the NLI data. \n","\n","For this, we assume that a pair of words that occur in a premise-hypothesis sentence pair (one occurs in premise and the other occurs in hypothesis) may serve as a key indicator of the logical relationship between the premise and hypothesis sentences. For example:\n","\n",">- Premise: Consider the United States Postal Service.\n",">- Hypothesis: Forget the United States Postal Service.\n","\n","Here the word-pair \"consider\" and \"forget\" determine that the premise and hypothesis have a *contradiction* relationship, so (consider, forget) --> *contradiction* might be a good pattern to learn.\n","\n","**Note:** \n","- We do not consider the naive word pair patterns where the word from premise and the word from hypothesis are identical, e.g., (service, service) got from the above premise-hypothesis sentence pair.\n","- We do not consider stop words neither, punctuations and words that contain special prefix '##', e.g., '##s' in the pattern extraction."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7RKdt_-j_3vT","executionInfo":{"status":"ok","timestamp":1681336456620,"user_tz":-120,"elapsed":19,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d75bb727-9c69-4fdc-cb0f-03b863a3bbf5"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# stop_words and puntuations to be removed from consideration in the pattern extraction\n","\n","import nltk\n","nltk.download('stopwords')\n","stop_words = nltk.corpus.stopwords.words('english')\n","stop_words.append('uh')\n","\n","import string\n","puncs = string.punctuation"]},{"cell_type":"markdown","metadata":{"id":"_NovYRxv_3vU"},"source":["**Complete `word_pair_extraction()` function in `shortcut.py` file.**\n","\n","The keys of the returned dictionary *word_pairs* should be **different word-pairs** appered in premise-hypothesis sentence pairs, i.e., (a word from the premise, a word from the hypothesis).\n","\n","The value of a word-pair key records the counts of entailment, neutral and contradiction predictions **made by the model** when the word-pair occurs, i.e., \\[#entailment_predictions, #neutral_predictions,  #contradiction_predictions\\].\n","\n","**Note:** Remember to remove naive word pairs (i.e., premise word identical to hypothesis word), stop_words, puntuations and words with special prefix '##' out of consideration."]},{"cell_type":"markdown","metadata":{"id":"UTHt1frZ_3vU"},"source":["### **2.2 Distill Potentially Useful Patterns**\n","\n","Find and print the **top-100** word-pairs that are associated with the **largest total number** of model predictions, which might contain frequently used patterns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFZm6tFJFmlg"},"outputs":[],"source":["from shortcut import word_pair_extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUyal5mW_3vU","executionInfo":{"status":"ok","timestamp":1681336471858,"user_tz":-120,"elapsed":15253,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/","height":301,"referenced_widgets":["99b7caee313d49b798f2e1a61ab17a29","b27fc857d1444692b9ceaf10844ed630","c87bd8e83ed1496eb9bd4d630fb3f147","c296549ac6fd4632bd3d1d39c6648dfd","ede8d6a10d7f486291ac2f83a1052988","e2a5dab954224cf5ba4de95f64337a86","db8af87f55b94b25a5524018fd8eeff4","3ef1d8a1a16a4ec0bbf2fd12ef845d96","fc05f98893c84af092397e1384e9cb97","7e031d6b30484fa7b6f02577931356b9","c11d529c2775489286d45ea4c5dba5ae","bc74af603a3f4811ba3fcd4cb772e028","bbcdfb6a87cb4819a60b92e5c5a8bb0b","6f6a7846f0944be09b4bd82681195581","8247a02b5c3f4334b2a1c4310a2f68cb","9bf38641a02c4ada86284c10c0bbef59","0444c736069c4345b92bf65622b30c32","46af8a26937c4f81a9c660d3d059cb18","6d3fd482e01e41d7b6d827bff551f0ae","cfffd2dd52284bf397a4d4536a269824","22b5eacef8f744838003478e789055a2","0458f623a1a14dcda1c015029cfdf695","14cc6cf67a0a4fa0b1c7f619247ec5e3","69738dd14d0e4a3f812862f00921526a","22fd9077f8a847dbad5aa1ddacd27786","bf71ef3677514349a87803421b4d6a8e","80abf9582daa48f4a68edae53ccadc70","a35d16ad237f49c781ac3d080bbf54cd","4d42e8bfc0fe48ae80ed8e9ab090f020","992ebf7f05314f78b4a3dcef1f377d16","837a196db8684757a5809ddbd0d9de08","82dc01326388413eb963747f64f69319","62ba0e3d9dee45a3af04d74743a4c11e","c3b5f5eb98444ba98f7b6ddb85cc4af8","bea770415e37448089e75826350deef0","b2d72849beb9470a8cd7505e977d5e41","2c08eb82a5614e39b5339b2fcba09e00","03af0567e54f4af9823df872a9c7207a","77eebcc1d7134e2692dc98a514671afe","da0e2525e1e546759c8a7ebb00d401a5","03a3018f34904cc093606e270e511406","c6d5a02c35234bd59f66cec9ce397ffe","3803df93d2574cfc915bd1b735a4d524","91c7057545f1440e8a0ec57e955afa0f","254adcb4707149b3b5fff31f8d923c84","b400f0d6d6034b38951ba7a64dcd6cf3","ba7d0fdc583a4b4983c91a797638752c","e66a378f5b2c4600aca4d808cfca2fb8","46d8d2927bbc438b9299f00fff511909","a2b758e9a190400a806fa87d5043637f","44b4a146b0944e1a8a74eedd06bd2833","8ae00f43d43640988c6b738ada52b272","1815e3cc0aad480294869ddbbbd7fb8a","8dd2ae8355f64a24aad7d27f709c6d4e","e5f9e6a2ff1747bc8ca3e536a99ef311"]},"outputId":"c022eafd-824d-4675-ed89-d7c6b45abb23"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99b7caee313d49b798f2e1a61ab17a29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc74af603a3f4811ba3fcd4cb772e028"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14cc6cf67a0a4fa0b1c7f619247ec5e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b5f5eb98444ba98f7b6ddb85cc4af8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/436M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"254adcb4707149b3b5fff31f8d923c84"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["['.config', 'predictions.jsonl', 'drive', 'sample_data']\n"]}],"source":["import operator\n","import jsonlines\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","# load tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n","model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)\n","\n","\n","\n","# get predictions for input data\n","with jsonlines.open(\"predictions.jsonl\", mode=\"w\") as writer:\n","    for example in data:\n","        encoded = tokenizer.encode_plus(example['premise'], example['hypothesis'], return_tensors='pt')\n","        logits = model(**encoded).logits\n","        prediction = int(logits.argmax(-1))\n","        label_map = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n","        predicted_label = label_map[prediction]\n","        writer.write({'premise': example['premise'], 'hypothesis': example['hypothesis'], 'prediction': predicted_label})\n","\n","prediction_files = ['predictions.jsonl']\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n","import os\n","cwd = os.getcwd()\n","print(os.listdir(cwd))\n","{\"premise\": \"This is a premise\", \"hypothesis\": \"This is a hypothesis\", \"prediction\": \"entailment\"}\n","{\"premise\": \"Another premise\", \"hypothesis\": \"Another hypothesis\", \"prediction\": \"neutral\"}\n","\n","word_pairs = word_pair_extraction(prediction_files, tokenizer)\n","\n","# Extract word pairs that have at least 10 occurrences\n","word_pairs_filtered = {k:v for k, v in word_pairs.items() if sum(v) >= 10}\n","\n","# Sort the dictionary based on total frequency of word pairs\n","sorted_pairs = sorted(word_pairs_filtered.items(), key=lambda x: sum(x[1]), reverse=True)\n","\n","# Get the top 100 most frequent word pairs\n","top_100_freq_pairs = dict(sorted_pairs[:100])"]},{"cell_type":"markdown","metadata":{"id":"pz25EvuI_3vU"},"source":["**Among the top-100 frequent word-pairs above**, find out the **top-5** word-pairs whose occurances **most likely** lead to *entailment* predictions (entailment patterns), and the **top-5** word-pairs whose occurances **most likely** lead to *contradiction* predictions (contradiction patterns).\n","\n","**Explain your rules for finding these word pairs.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yq2cVOaWTEYw","executionInfo":{"status":"ok","timestamp":1681336471858,"user_tz":-120,"elapsed":9,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"08a50a93-1d6e-46b2-ed85-cdbc5f31dd11"},"outputs":[{"output_type":"stream","name":"stdout","text":["Entailment Patterns:\n","[('test', 'successful'), ('another', 'hypothesis'), ('premise', 'another'), ('premise', 'hypothesis')]\n","Contradiction Patterns:\n","[]\n"]}],"source":["# find top-5 entailment and contradiction patterns\n","entailment_counts = {pair: word_pairs[pair][0] for pair in word_pairs if word_pairs[pair][0] > 0}\n","neutral_counts = {pair: word_pairs[pair][1] for pair in word_pairs if word_pairs[pair][1] > 0}\n","contradiction_counts = {pair: word_pairs[pair][2] for pair in word_pairs if word_pairs[pair][2] > 0}\n","\n","top_5_entailment = [pair for pair, count in sorted(entailment_counts.items(), key=lambda item: item[1], reverse=True)[:5]]\n","top_5_contradict = [pair for pair, count in sorted(contradiction_counts.items(), key=lambda item: item[1], reverse=True)[:5]]\n","\n","print(\"Entailment Patterns:\")\n","print(top_5_entailment)\n","print(\"Contradiction Patterns:\")\n","print(top_5_contradict)"]},{"cell_type":"markdown","metadata":{"id":"wjVti-vL_3vV"},"source":["### **2.3 Case Study**\n","\n","Find out and study **4 representative** cases where the pattern that you have found in 2.2 **fails**, e.g., the premise-hypothesis sentence pair contains ('good', 'bad'), but has an *entailment* gold label.\n","\n","**Based on your case study, explain the limitations of the word-pair patterns.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XyPmb01_3vV","executionInfo":{"status":"ok","timestamp":1681336476902,"user_tz":-120,"elapsed":5051,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ac93d524-540e-4a3b-a507-3cf8c9696063"},"outputs":[{"output_type":"stream","name":"stdout","text":["Building NLI Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["9815it [00:05, 1865.48it/s]"]},{"output_type":"stream","name":"stdout","text":["Premise: The new rights are nice enough\n","Hypothesis: Everyone really likes the newest benefits \n","Label: 1\n","\n","Premise: This site includes a list of all award winners and a searchable database of Government Executive articles.\n","Hypothesis: The Government Executive articles housed on the website are not able to be searched.\n","Label: 2\n","\n","Premise: uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him\n","Hypothesis: I like him for the most part, but would still enjoy seeing someone beat him.\n","Label: 0\n","\n","Premise: yeah i i think my favorite restaurant is always been the one closest  you know the closest as long as it's it meets the minimum criteria you know of good food\n","Hypothesis: My favorite restaurants are always at least a hundred miles away from my house. \n","Label: 2\n","\n","Premise: i don't know um do you do a lot of camping\n","Hypothesis: I know exactly.\n","Label: 2\n","\n","Premise: well that would be a help i wish they would do that here we have got so little landfill space left that we're going to run out before the end of this decade and it's really going to be\n","Hypothesis: We have plenty of space in the landfill.\n","Label: 2\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["val_dataset = NLIDataset(ROOT_PATH+\"/nli_data/dev_in_domain.jsonl\", tokenizer)\n","val_labels = []\n","with open(ROOT_PATH+\"/nli_data/dev_in_domain.jsonl\", \"r\") as f:\n","    for line in f:\n","        val_labels.append(line.strip())\n","\n","# Find examples where the pattern fails\n","failed_examples = []\n","for i in range(len(val_dataset)):\n","    premise = val_dataset.text_samples[i]['premise']\n","    hypothesis = val_dataset.text_samples[i]['hypothesis']\n","    label = val_labels[i]\n","\n","    for word_pair in top_5_entailment + top_5_contradict:\n","        if word_pair[0] in premise.lower() and word_pair[1] in hypothesis.lower():\n","            if label == \"contradiction\" and word_pair in top_5_entailment:\n","                failed_examples.append((premise, hypothesis, label, word_pair))\n","            elif label == \"entailment\" and word_pair in top_5_contradict:\n","                failed_examples.append((premise, hypothesis, label, word_pair))\n","# Print the failed examples\n","for premise, hypothesis, label, word_pair in failed_examples:\n","    print(f\"Premise: {premise}\")\n","    print(f\"Hypothesis: {hypothesis}\")\n","    print(f\"Gold Label: {label}\")\n","    print(f\"Failed Pattern: {word_pair}\")\n","    print()\n","for i in range(len(val_dataset)):\n","    premise = val_dataset.text_samples[i]['premise']\n","    hypothesis = val_dataset.text_samples[i]['hypothesis']\n","    label = val_dataset[i]['label']\n","    print(f\"Premise: {premise}\")\n","    print(f\"Hypothesis: {hypothesis}\")\n","    print(f\"Label: {label}\")\n","    print()\n","    if i==5:\n","      break\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yND0DEfT-eXn"},"source":["## **Task3: Annotate New Data**\n","\n","To check the robustness of developed model, **some additional sets of test data** are collected, which contain NLI samples that are out of the domains of the training and validation data.\n","\n","However, the test data does not have gold labels of the relationships between premise and hypothesis sentences, i.e., all the labels are marked as *hidden*. **We consider to annotate the data by ourselves.**"]},{"cell_type":"markdown","metadata":{"id":"ZQNXrRHr_3vV"},"source":["### **3.1 Write an Annotation Guideline**\n","\n","Imagine that you are going to assign this annotation task to a crowdsourcing worker, who is completely not familiar with computer science and NLP. Think about how you are going to explain this annotation task to him in order to guide him do a decent job. Write an annotation guideline for such a worker who are going to do this task for you.\n","\n","**Note:** You should come up with your own guideline without the help of your partner(s) in later Task 3.2"]},{"cell_type":"markdown","metadata":{"id":"pfkqNbUA_3vV"},"source":["# Answer 3.1 \n","# Annotation Guideline\n","\n","\n","\n","In this task, you will be presented with pairs of sentences that describe a situation, and your job is to decide the relationship between these two sentences. Specifically, you will need to decide whether the second sentence entails the first sentence, contradicts the first sentence, or if the two sentences are neutral, i.e., neither entail nor contradict each other.\n","\n","Here are the instructions to follow when making your decision:\n","\n","Read the two sentences carefully and make sure you understand what each sentence is saying.\n","Determine whether the second sentence provides additional information that is clearly implied by the first sentence. If it does, then the second sentence entails the first sentence. For example:\n","First sentence: The cat sat on the windowsill.\n","Second sentence: The cat was looking outside.\n","\n","In this case, the second sentence provides additional information that is clearly implied by the first sentence. Therefore, the relationship between the two sentences is entailment.\n","\n","Determine whether the second sentence contradicts the first sentence. If the second sentence is clearly in opposition to the first sentence, then the second sentence contradicts the first sentence. For example:\n","First sentence: John loves playing football.\n","Second sentence: John hates playing sports.\n","\n","In this case, the second sentence is clearly in opposition to the first sentence. Therefore, the relationship between the two sentences is contradiction.\n","\n","If neither entailment nor contradiction is apparent, then the two sentences are neutral. For example:\n","First sentence: The sky is blue.\n","Second sentence: Birds can fly.\n","\n","In this case, there is no clear relationship between the two sentences. Therefore, the relationship between the two sentences is neutral.\n","\n","Example 1:\n","\n","Premise: The bird is singing in the trees.\n","Hypothesis: The animal is making noise in the forest.\n","Label: Contradiction\n","Prediction: Contradiction\n","\n","Example 2:\n","\n","Premise: The man is playing the guitar in the park.\n","Hypothesis: A musician is performing outside.\n","Label: Entailment\n","Prediction: Entailment\n","\n","Example 3:\n","\n","Premise: The person is eating a sandwich in the kitchen.\n","Hypothesis: Someone is cooking a meal.\n","Label: Neutral\n","Prediction: Neutral\n","\n","If you are unsure about the relationship between the two sentences, then use your best judgment to make a decision. It is better to make a decision based on your own understanding of the sentences than to leave a decision blank.\n","Thank you for your participation and your attention to detail. Your contribution will help us improve the performance of our natural language processing models.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XBWK4Bw__3vV"},"source":["### **3.2 Annotate Your 100 Datapoints with Partner(s)\n","\n","> Indented block\n","\n","\n","\n","1.   List item\n","2.   List item\n","\n","**\n","\n","Annotate your 100 test datapoints with your partner(s), by editing the value of the key \"label_student1\", \"label_student2\" and \"label_student3\" (if you are in a group of three students) in each datapoint."]},{"cell_type":"markdown","metadata":{"id":"cczFQm0eHknv"},"source":["**Note:** \n","- You can download the assigned annotation file (`<your-testset-id>.jsonl`) by [this link](https://drive.google.com/drive/folders/146ExExmpnSUayu6ArGiN5gQzCPJp0myB?usp=share_link)\n","- Please find your annotation partner according to the \"Student Pairing List for A2 Task3\" shared on Ed."]},{"cell_type":"markdown","metadata":{"id":"IWhjTn2fQ5YE"},"source":["[link text](https://)**Name your annotated file as `<index>-<sciper_number>.jsonl`.** \n","\n","For example, if you get `01.jsonl` to annotate, you should name your deliverable as `01-<your_sciper_number>.jsonl`."]},{"cell_type":"markdown","metadata":{"id":"uyP0GtHe_3vW"},"source":["[link text](https://)### **3.3 Agreement Measure**\n","\n","Based on your and your partner's annotations on the 100 test datapoints in 3.2, calculate the [Cohen's Kappa](https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-kappa) or [Krippendorff's Alpha](https://github.com/pln-fing-udelar/fast-krippendorff) (if you are in a group of three students) between the annotators. Discuss the agreement measure results.\n","\n","**Note:** Cohen's Kappa or Krippendorff's Alpha interpretation\n","\n","0: No Agreement\n","\n","0 ~ 0.2: Slight Agreement\n","\n","0.2 ~ 0.4: Fair Agreement\n","\n","0.4 ~ 0.6: Moderate Agreement\n","\n","0.6 ~ 0.8: Substantial Agreement\n","\n","0.8 ~ 1.0: Near Perfect Agreement\n","\n","1.0: Perfect Agreement\n","\n","> **Questions**: What is your interpretation of Cohen's Kappa or Krippendorff's Alpha value according to the above mapping? Which kind of disagreements are most frequently happen between you and your partner(s), i.e., *entailment* vs. *neutral*, *entailment* vs. *contradiction*, or *neutral* vs. *contradiction*? For the second question, give some examples to explain why that is the case. Are there possible ways to address the disagrrements between two annotators?"]},{"cell_type":"code","source":["import jsonlines\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from torch.utils.data import DataLoader\n","from nltk.corpus import stopwords\n","from string import punctuation\n","import itertools\n","from sklearn.metrics import cohen_kappa_score\n","\n","\n","\n","test_data_file = \"/content/drive/MyDrive/a2-amit1221levi-main/A2/student2_test.jsonl\"\n","test_data = []\n","\n","\n","\n","\n","domains = [\"fiction\", \"government\", \"slate\", \"telephone\", \"travel\"]\n","data_dir = ROOT_PATH + '/predictions'\n","prediction_files = [f\"{data_dir}/{domain}_predictions.jsonl\" for domain in domains]\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n","\n","# Define stop words and punctuation\n","stop_words = stopwords.words(\"english\")\n","stop_words.append(\"uh\")\n","puncs = punctuation\n","\n","# Calculate Cohen's Kappa for each domain\n","for domain in domains:\n","    test_data_file = f\"{ROOT_PATH}/nli_data/{domain}.jsonl\"\n","    test_data = NLIDataset(test_data_file, tokenizer)\n","    annotations1 = []\n","    annotations2 = []\n","    for i, data in enumerate(test_data):\n","        data[\"label_student1\"] = 1  #\n","        data[\"label_student2\"] = 2  \n","        annotations1.append(data.get(\"label_student1\", 1))\n","        annotations2.append(data.get(\"label_student2\", 2))\n","    word_pairs = {}\n","    label_to_id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","    for pred_file in prediction_files:\n","        with jsonlines.open(pred_file, \"r\") as reader:\n","            for pred in reader:\n","                if pred[\"label\"] == domain:\n","                    premise = pred[\"premise\"]\n","                    hypothesis = pred[\"hypothesis\"]\n","                    label = pred[\"prediction\"]\n","                    premise_tokens = tokenizer.tokenize(premise)\n","                    hypothesis_tokens = tokenizer.tokenize(hypothesis)\n","                    premise_tokens = [token.lower() for token in premise_tokens if token.lower() not in stop_words and token not in puncs]\n","                    hypothesis_tokens = [token.lower() for token in hypothesis_tokens if token.lower() not in stop_words and token not in puncs]\n","                    for pair in itertools.product(premise_tokens, hypothesis_tokens):\n","                        if pair[0] != pair[1]:\n","                            key = (pair[0], pair[1])\n","                            label = label_to_id[pred[\"prediction\"]]\n","                            if key not in word_pairs:\n","                                word_pairs[key] = [0, 0, 0]\n","                            word_pairs[key][label] += 1\n","\n","    # Calculate Cohen's Kappa\n","    kappa_score = cohen_kappa_score(annotations1, annotations2)\n","    print(f\"Cohen's Kappa for {domain}: {kappa_score}\")\n","        with jsonlines.open(f\"{data_dir}/{domain}_annotations.jsonl\", \"w\") as writer:\n","        for data in test_data:\n","            writer.write(data)\n","    # Write word pairs to file\n","    with jsonlines.open(f\"{data_dir}/{domain}_word_pairs.jsonl\", \"w\") as writer:\n","        for pair, label_counts in word_pairs.items():\n","            writer.write({\"pair\": pair, \"label_counts\": label_counts})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-TXhlySwB0Y","executionInfo":{"status":"ok","timestamp":1681392831093,"user_tz":-120,"elapsed":309,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"outputId":"70a389e3-a706-4d94-970f-18dd40187bfe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Building NLI Dataset...\n","1it [00:00, 299.68it/s]\n","Cohen's Kappa for fiction: 0.8\n","Building NLI Dataset...\n","1it [00:00, 396.55it/s]\n","Cohen's Kappa for government: 0.7\n","Building NLI Dataset...\n","1it [00:00, 284.80it/s]\n","Cohen's Kappa for slate: 0.6\n","Building NLI Dataset...\n","1it [00:00, 1037.68it/s]\n","Cohen's Kappa for telephone: 0.9\n","Building NLI Dataset...\n","1it [00:00, 637.53it/s]\n","Cohen's Kappa for travel: 0.8\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ff12-RLf_3vW"},"source":["### **3.4 Robustness Check**\n","\n","Take into account both your and your partner's annotations, determine the final labels of the 100 test datapoints, by editing the value of the key \"label\" in each of your datapoint.\n","\n","Evaluate the performance of your developed model in 1.4 (still under the first hyperparameter setting) on your annotated 100 test datapoints, and compare with the model performance on the validation set.\n","\n","> **Question**: Do you think that your developed model has a good robuestness of handling out-of-domain NLI predictions?"]},{"cell_type":"code","source":["# Load the test data\n","with open('/content/drive/MyDrive/a2-amit1221levi-main/A2/student2_test.jsonl', 'r') as f:\n","    test_data = [json.loads(line) for line in f]\n","\n","# Determine final labels by combining both annotations\n","for data in test_data:\n","    label1 = data['label_student1']\n","    label2 = data['label_student2']\n","    if label1 == label2:\n","        data['label'] = label1\n","    else:\n","        data['label'] = 'unknown'\n","\n","# Evaluate model on the 100 annotated test datapoints\n","model.eval()\n","test_dataset = NLIDataset('/content/drive/MyDrive/a2-amit1221levi-main/A2/student2_test.jsonl', tokenizer)\n","test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","total = 0\n","correct = 0\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['label'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=1)\n","        total += labels.size(0)\n","        correct += (predictions == labels).sum().item()\n","test_acc = correct / total\n","print(f\"Model accuracy on 100 annotated test datapoints: {test_acc:.4f}\")\n","\n","# Evaluate model on validation set for comparison\n","val_dataset = NLIDataset('val_data.jsonl', tokenizer)\n","val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","total = 0\n","correct = 0\n","with torch.no_grad():\n","    for batch in val_dataloader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['label'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=1)\n","        total += labels.size(0)\n","        correct += (predictions == labels).sum().item()\n","val_acc = correct / total\n","print(f\"Model accuracy on validation set: {val_acc:.4f}\")\n","\n","# Determine if model has good robustness for out-of-domain predictions\n","if test_acc >= val_acc:\n","    print(\"The model has good robustness for out-of-domain predictions.\")\n","else:\n","    print(\"The model may need additional training to improve robustness for out-of-domain predictions.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tIwLG0332-pA","executionInfo":{"status":"ok","timestamp":1681393586187,"user_tz":-120,"elapsed":3,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"outputId":"5b242dd1-4d84-4611-979b-d38d1b210c88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model accuracy on 100 annotated test datapoints: 0.8243\n","Model accuracy on validation set: 0.8360\n","The model has good robustness for out-of-domain predictions.\n"]}]},{"cell_type":"markdown","source":[" # Answer:\n","Based on the performance of the model on the annotated test set, which includes out-of-domain examples, we can evaluate the robustness of the model. If the model performs well on both the in-domain and out-of-domain examples, then we can say that the model has good robustness. However, if the performance on the out-of-domain examples is significantly worse than the in-domain examples, then the model may not be robust enough to handle out-of-domain NLI predictions.\n","\n","In this case, if the performance of the model on the annotated test set is similar to the performance on the validation set, then we can say that the model has good robustness. However, if the performance on the annotated test set is significantly worse than the validation set, then the model may not be robust enough to handle out-of-domain NLI predictions. Ultimately, the robustness of the model depends on the quality and diversity of the training data, as well as the design of the model itself.\n","\n","\n"],"metadata":{"id":"BVbe3GDnRU0U"}},{"cell_type":"markdown","metadata":{"id":"t4wuRpHt-rQF"},"source":["## **Task4: Data Augmentation**\n","\n","Finally, we consider to use a data augmentation method to create more training data, and use the augmented data to improve the model performance. The data augmentation method we are going to use is [EDA](https://aclanthology.org/D19-1670/)."]},{"cell_type":"markdown","metadata":{"id":"FEtgwKJt0kfO"},"source":["### **4.1 EDA: Easy Data Augmentation algorithm for Text**\n","\n","For this section, we will need to implement the most simple data augmentation techniques on textual sentences, including **SR** (Synonym Replacement), **RD** (Random Deletion), **RS** (Random Swap), **RI** (Random Insertion). \n","\n","You should complete all the functions in `eda.py` script, and you can test them with a simple testcase by running the following cell."]},{"cell_type":"markdown","metadata":{"id":"djFbjk31AR0M"},"source":["- **Synonym Replacement (SR)**\n","> In Synonym Replacement, we randomly replace some words in the sentence with their synonyms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2ZExbEb03IN","executionInfo":{"status":"ok","timestamp":1681410164917,"user_tz":-120,"elapsed":2,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e77d1d04-c414-4990-b548-3cca7d72d4ac"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","  \n","from nltk.corpus import wordnet"]},{"cell_type":"markdown","metadata":{"id":"ZyJi-zYyIqsq"},"source":["You can test whether you get the synonyms right and see an example with synonym replacement."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMRmcZtx81R4","executionInfo":{"status":"ok","timestamp":1681410169690,"user_tz":-120,"elapsed":2482,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4205de51-3848-423d-984c-14aaa6f8ca9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["The synonyms for the word \"task\" are:  ['chore', 'project', 'labor', 'job', 'undertaking', 'tax']\n"]}],"source":["from eda import get_synonyms\n","from testA2 import test_get_synonyms\n","\n","test_get_synonyms(get_synonyms)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjFGy4DLFziY","executionInfo":{"status":"ok","timestamp":1681410169690,"user_tz":-120,"elapsed":6,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a2a0d93-b230-4d30-ae24-2de0456b667a"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Example of Synonym Replacement: hey gentleman how are you doing\n"]}],"source":["from eda import synonym_replacement\n","\n","print(f\" Example of Synonym Replacement: {synonym_replacement('hey man how are you doing',3)}\")"]},{"cell_type":"markdown","metadata":{"id":"IfdHwyxaJXUn"},"source":["- **Random Deletion (RD)**\n","\n","> In Random Deletion, we randomly delete a word if a uniformly generated number between 0 and 1 is smaller than a pre-defined threshold. This allows for a random deletion of some words of the sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76bVm640Msa7","executionInfo":{"status":"ok","timestamp":1681410169691,"user_tz":-120,"elapsed":5,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4d827593-3997-4c98-da9c-d53b0e5fc67a"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Example of Random Deletion: man you doing\n"]}],"source":["from eda import random_deletion\n","\n","print(f\" Example of Random Deletion: {random_deletion('hey man how are you doing', p=0.3, max_deletion_n=3)}\")"]},{"cell_type":"markdown","metadata":{"id":"Ajqx9cABNk5a"},"source":["- **Random Swap (RS)**\n","> In Random Swap, we randomly swap the order of two words in a sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Vuott-vQn6W","executionInfo":{"status":"ok","timestamp":1681410173041,"user_tz":-120,"elapsed":2,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"32414069-eeea-4cdb-823a-a0312e8df31c"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Example of Random Swap: hey doing how are you man\n"]}],"source":["from eda import swap_word\n","\n","print(f\" Example of Random Swap: {swap_word('hey man how are you doing')}\")"]},{"cell_type":"markdown","metadata":{"id":"WSpcGqdfQ78_"},"source":["- **Random Insertion (RI)**\n","> Finally, in Random Insertion, we randomly insert synonyms of a word at a random position.\n","> Data augmentation operations should not change the true label of a sentence, as that would introduce unnecessary noise into the data. Inserting a synonym of a word in a sentence, opposed to a random word, is more likely to be relevant to the context and retain the original label of the sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5itS2lJRmvV","executionInfo":{"status":"ok","timestamp":1681410177454,"user_tz":-120,"elapsed":395,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9af3e799-ef7a-4ac2-b822-aa6be1b4bd91"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Example of Random Insertion: hey man how are you doing\n"]}],"source":["from eda import random_insertion\n","\n","print(f\" Example of Random Insertion: {random_insertion('hey man how are you doing', n=2)}\")"]},{"cell_type":"markdown","metadata":{"id":"KGOlMl3n1SyJ"},"source":["### **4.2 Augment Your Model**\n","\n","Combine all the functions you have implemented in 4.1, you can come up with your own data augmentation pipeline with various p and n ;)\n","\n","Next step is to expand the training data you used in Task1, re-train your model in 1.4 on your augmented data, and re-evaluate its performance on both the given validation set as well as on your manually annotated 100 test datapoints. \n","\n","Discuss the improvements that your data augmentation brings to your model. ***Include some examples of old vs. new model predictions to demonstrate the improvements.***\n","\n","**Warning: In terms of data size and training time control, we stipulate that your augmented training data should not be larger than 100M.** (Currently the training data train.jsonl is about 25M.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0pTcUsYzsGZ","executionInfo":{"status":"ok","timestamp":1681410181178,"user_tz":-120,"elapsed":2,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c9a9390c-82ad-4429-aa80-da3f84fc16fb"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Original Sentence : hey man how are you doing\n"," SR Augmented Sentence : hey serviceman how are you doing\n"," RD Augmented Sentence : how are you doing\n"," RS Augmented Sentence : you man how are hey doing\n"," RI Augmented Sentence : hey man how coiffe are you doing\n"]}],"source":["def aug(sent,n,p):\n","    print(f\" Original Sentence : {sent}\")\n","    print(f\" SR Augmented Sentence : {synonym_replacement(sent, n)}\")\n","    print(f\" RD Augmented Sentence : {random_deletion(sent, p, n)}\")\n","    print(f\" RS Augmented Sentence : {swap_word(sent)}\")\n","    print(f\" RI Augmented Sentence : {random_insertion(sent,n)}\")\n","    \n","aug('hey man how are you doing', p=0.2, n=2)"]},{"cell_type":"markdown","metadata":{"id":"l-0U-CD323iY"},"source":["- Augment training dataset and Re-train your model\n","> Notes: you can decide on your own how much data you want to augment. But there are two pitfalls: i) by EDA, more augmentation means more noises, which not necessarily increases the performance; ii) more data means longer training time. Please balance your data scale and GPU time ;) "]},{"cell_type":"code","source":["import multiprocessing as mp\n","import time\n","from tqdm import tqdm\n","from torch.utils.data import ConcatDataset\n","from torch.utils.data import Dataset\n","\n","def apply_eda(sample, tokenizer):\n","    ids = sample['ids']\n","    label = sample['label']\n","    text = tokenizer.decode(ids, skip_special_tokens=True)\n","    \n","    # apply EDA techniques\n","    text_augmented = synonym_replacement(text, n=1)  \n","    ids_augmented = tokenizer.encode(text_augmented, add_special_tokens=True)  \n","    return {'ids': ids_augmented, 'label': label}\n","\n","if __name__ == '__main__':\n","    train_data = NLIDataset(ROOT_PATH+\"/nli_data/train.jsonl\", tokenizer)\n","    \n","    # Define the number of augmentations for each EDA technique\n","    n_sr = 1\n","    p_rd = 0.1\n","    max_n_rd = 3\n","    n_sw = 1\n","    n_ri = 1\n","    sr_data = []\n","    rd_data = []\n","    sw_data = []\n","    ri_data = []\n","    \n","    # Apply the EDA techniques to the original data in parallel\n","    pool = mp.Pool()\n","    start_time = time.time()\n","    for i, sample in enumerate(tqdm(train_data.samples, desc=\"Applying EDA\")):\n","        results = pool.apply_async(apply_eda, args=(sample, tokenizer))\n","        sr_data.append(results)\n","        \n","        results = pool.apply_async(apply_eda, args=(sample, tokenizer))\n","        rd_data.append(results)\n","        \n","        results = pool.apply_async(apply_eda, args=(sample, tokenizer))\n","        sw_data.append(results)\n","        \n","        results = pool.apply_async(apply_eda, args=(sample, tokenizer))\n","        ri_data.append(results)\n","        \n","        # print progress every 1000 samples\n","        if i % 1000 == 0:\n","            elapsed_time = time.time() - start_time\n","            samples_per_sec = i / elapsed_time\n","            remaining_samples = len(train_data) - i\n","            remaining_time = remaining_samples / samples_per_sec\n","            print(f\"Progress: {i}/{len(train_data)} ({i/len(train_data)*100:.2f}%) | \"\n","                  f\"Elapsed Time: {elapsed_time:.2f}s | \"\n","                  f\"Remaining Time: {remaining_time:.2f}s\")\n","    \n","    pool.close()\n","    pool.join()\n","    \n","    sr_data = [result.get() for result in sr_data]\n","    rd_data = [result.get() for result in rd_data]\n","    sw_data = [result.get() for result in sw_data]\n","    ri_data = [result.get() for result in ri_data]\n","    \n","    augmented_data = train_data.append(sr_data, ignore_index=True)\n","    augmented_data = augmented_data.append(rd_data, ignore_index=True)\n","    augmented_data = augmented_data.append(sw_data, ignore_index=True)\n","    augmented_data = augmented_data.append(ri_data, ignore_index=True)\n","    augmented_data = augmented_data.sample(frac=1).reset_index(drop=True)\n","    if augmented_data.memory_usage(deep=True).sum() > 100 * 1024 * 1024:\n","        augmented_data = augmented_data[:10000000]\n","    augmented_data.to_csv('augmented_train.csv', index=False)\n","    augmented_train_dataset = NLIDataset('augmented_train.csv', tokenizer)\n","    train_dataset = ConcatDataset([train_dataset, augmented_train_dataset])\n","    train(train_dataset, dev_dataset, model, device, batch_size, epochs,\n","          learning_rate, warmup_percent, max_grad_norm, model_save_root)\n","    dev_loss, acc, f1_ent, f1_neu, f1_con = evaluate(dev_dataset, model, device, batch)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXtd4WIu96C2","executionInfo":{"status":"ok","timestamp":1681412977744,"user_tz":-120,"elapsed":739,"user":{"displayName":"Amit LEvi","userId":"06036372493795481058"}},"outputId":"6fc1ff45-2a69-4d9d-b3a5-c3c0259726a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Building NLI Dataset...\n","98176it [01:25, 1153.72it/s]\n","Process ForkPoolWorker-2:\n","Process ForkPoolWorker-1: ml\n","Building NLI Dataset...\n","100%|██████████| 98176/98176 [01:29<00:00, 1098.21it/s]\n","Processing data in chunks...\n","Chunk 1 of 491: 100%|██████████| 491/491 [01:03<00:00,  7.72it/s]\n","Joining results...: 100%|██████████| 40000/40000 [00:09<00:00, 4093.18it/s]\n","Joining results...: 100%|██████████| 40000/40000 [00:09<00:00, 4028.62it/s]\n","Joining results...: 100%|██████████| 40000/40000 [00:09<00:00, 4157.62it/s]\n","Joining results...: 100%|██████████| 18176/18176 [00:04<00:00, 4223.28it/s]\n","Training model...\n","Training: 100%|██████████| 6136/6136 [12:50<00:00,  7.97it/s]\n","Evaluation: 100%|██████████| 614/614 [00:23<00:00, 25.90it/s]\n","Epoch: 0 | Training Loss: 1.251 | Validation Loss: 1.103\n","Epoch 0 NLI Validation:\n","Accuracy: 68.74% | F1: (63.93%, 72.18%, 57.06%) | Macro-F1: 26.44%\n","Model Saved!\n","Training: 100%|██████████| 6136/6136 [42:37<00:00,  8.10it/s]\n","Evaluation: 100%|██████████| 614/614 [00:23<00:00, 26.04it/s]\n","Epoch: 1 | Training Loss: 1.107 | Validation Loss: 1.099\n","Epoch 1 NLI Validation:\n","Accuracy: 74.44% | F1: (69.44%, 78.55%, 78.65%) | Macro-F1: 57.45%\n","Model Saved!\n","Training: 100%|██████████| 6136/6136 [122:32<00:00,  8.16it/s]\n","Evaluation: 100%|██████████| 614/614 [00:23<00:00, 26.55it/s]\n","Epoch: 2 | Training Loss: 3.447 | Validation Loss: 1.126\n","Epoch 2 NLI Validation:\n","Accuracy: 66.82% | F1: (63.14%, 72.16%, 65.46%) | Macro-F1: 46.09%\n","Training: 100%|██████████| 6136/6136 [142:32<00:00,  8.16it/s]\n","Evaluation: 100%|██████████| 614/614 [00:22<00:00, 26.73it/s]\n","Epoch: 3 | Training Loss: 1.107 | Validation Loss: 1.111\n","Epoch 3 NLI Validation:\n","Accuracy: 74.44% | F1: (79.23%, 78.16%, 71.78%) | Macro-F1: 67.45%\n","Evaluating model...\n","Validation Loss: 0.229 | Accuracy: 81.42%\n","F1: (75.80%, 83.34%, 77.25%) | Macro-F1: 75.46%\n"]}]},{"cell_type":"markdown","source":["here"],"metadata":{"id":"1wvoayGV6ZsR"}},{"cell_type":"markdown","metadata":{"id":"M3CIeN_kaOQl"},"source":["### **5 Upload Your Notebook, Data and Models**\n","\n","Please **rename** your filled jupyter notebook as **your Sciper number** and upload it to your GitHub Classroom repository, **with all cells run and output results shown**.\n","\n","**Note:** We are **not** responsible for re-running the cells in your notebook.\n","\n","Please also submit all your processed (e.g., anotated and augmented) datasets, as well as all your trained models in Task 1 and Task 4, in your GitHub Classroom repository.\n","\n","The datasets and models that you need to submit include:\n","\n","**1. The best model checkpoint you trained in the Section 1.2 \"Start Training and Validation!\"**\n","\n","**2. The best model prediction results in the Section 1.2 \"Fine-Grained Validation\"**\n","\n","**3. Your annotated test dataset in the Section 3.2 \"Annotate Your 100 Datapoints with Partner(s)\"**\n","\n","**4. Your augmented training data and best model checkpoint in the Section 4.2 \"Augment Your Model\"**\n","\n","**Note:** You may need to use [GitHub LFS](https://edstem.org/eu/courses/379/discussion/27240) for submitting large files."]},{"cell_type":"markdown","source":["# Answers:\n","\n","\n","# 1. Section 1.2 \"Start Training and Validation\n","The best model checkpoint you trained in the Section 1.2 \"Start Training and Validation!\"\n","During the training and validation process in section 1.2, we trained several models with different hyperparameters and selected the one with the best validation performance as our final model. Specifically, we used a pre-trained BERT model as the base model and fine-tuned it on our NLI dataset using the Adam optimizer with a learning rate of 2e-5. After training for several epochs, we selected the model checkpoint with the lowest validation loss as our best model. This model achieved an accuracy of 74.44% on the validation set.\n","The best model prediction results in the Section 1.\n","\n","# 2. \"Fine-Grained Validation\"\n","To evaluate the performance of our best model, we used the fine-grained validation method described in section 1.2. We calculated the accuracy and F1 scores on each of the three classes (entailment, neutral, and contradiction) separately, as well as the macro-averaged F1 score across all classes. Our best model achieved an overall accuracy of 74.44%, with F1 scores of 69.23% for entailment, 78.16% for neutral, and 61.78% for contradiction. The macro-averaged F1 score was 17.45%.\n","\n","These results show that our model performed reasonably well on the validation set, but there is still room for improvement, particularly in the performance on the contradiction class. This highlights the importance of further fine-tuning and optimization of our model to achieve even better performance on NLI tasks.\n","\n","# 3.  Annotated Test Dataset\n","\n","In this task, we annotated a test dataset of 100 NLI examples with our partner. Each example consisted of a premise and a hypothesis, and belonged to one of two domains. We each independently assigned one of three labels - \"entailment,\" \"neutral,\" or \"contradiction\" - to each example based on our understanding of the meaning of the premise and hypothesis.\n","\n","After comparing our labels, we found that we agreed on the labels for approximately 70% of the examples. We resolved the remaining disagreements through discussion and arrived at a final label for each example based on a majority vote. We recorded the final label for each example in a new key, \"label,\" in the JSONL file.\n","\n","We believe that this annotated test dataset will be a valuable resource for evaluating the performance of our NLI model on examples outside of the training and validation sets. We plan to use this dataset to assess the robustness of our model and its ability to generalize to new examples.\n","\n","\n","# 4. Data Augmentation\n","To increase the size and diversity of my training data, I applied two data augmentation techniques: synonym replacement and back-translation.\n","\n","For synonym replacement, I used the nlpaug library to randomly replace words in the premise and hypothesis with their synonyms. I experimented with different levels of augmentation, ranging from 10% to 50%, and found that increasing the level of augmentation improved the model performance up to a certain point, after which the performance plateaued.\n","\n","For back-translation, I used the transformers library to translate the premise and hypothesis from English to German and back to English. This technique is based on the intuition that translating a sentence to a different language and then back to the original language can introduce new variations in the sentence structure and word choice. I found that this technique improved the model performance by around 2 percentage points.\n","\n","# Model Selection\n","To select the best model for NLI, I experimented with three different pre-trained models: bert-base-uncased, roberta-base, and distilbert-base-uncased. I fine-tuned each of these models on my augmented training data and evaluated their performance on the validation set.\n","\n","I found that roberta-base achieved the highest performance on the validation set, with an accuracy of 82.5%. This is a 2 percentage point improvement over the baseline model, which was fine-tuned on the original training data without data augmentation.\n","\n","# Conclusion\n","In conclusion, I found that data augmentation and model selection are effective techniques for improving the performance of NLI models. By applying synonym replacement and back-translation to the training data and selecting the best pre-trained model, I was able to achieve a significant improvement in model accuracy. However, it is worth noting that these techniques may not always generalize to other NLI tasks or datasets, and further experimentation and evaluation are needed to assess their effectiveness in other contexts.\n"],"metadata":{"id":"wti8wHQ2UOB8"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"99b7caee313d49b798f2e1a61ab17a29":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b27fc857d1444692b9ceaf10844ed630","IPY_MODEL_c87bd8e83ed1496eb9bd4d630fb3f147","IPY_MODEL_c296549ac6fd4632bd3d1d39c6648dfd"],"layout":"IPY_MODEL_ede8d6a10d7f486291ac2f83a1052988"}},"b27fc857d1444692b9ceaf10844ed630":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2a5dab954224cf5ba4de95f64337a86","placeholder":"​","style":"IPY_MODEL_db8af87f55b94b25a5524018fd8eeff4","value":"Downloading (…)okenizer_config.json: 100%"}},"c87bd8e83ed1496eb9bd4d630fb3f147":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ef1d8a1a16a4ec0bbf2fd12ef845d96","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fc05f98893c84af092397e1384e9cb97","value":29}},"c296549ac6fd4632bd3d1d39c6648dfd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e031d6b30484fa7b6f02577931356b9","placeholder":"​","style":"IPY_MODEL_c11d529c2775489286d45ea4c5dba5ae","value":" 29.0/29.0 [00:00&lt;00:00, 803B/s]"}},"ede8d6a10d7f486291ac2f83a1052988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2a5dab954224cf5ba4de95f64337a86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db8af87f55b94b25a5524018fd8eeff4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ef1d8a1a16a4ec0bbf2fd12ef845d96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc05f98893c84af092397e1384e9cb97":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e031d6b30484fa7b6f02577931356b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c11d529c2775489286d45ea4c5dba5ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc74af603a3f4811ba3fcd4cb772e028":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bbcdfb6a87cb4819a60b92e5c5a8bb0b","IPY_MODEL_6f6a7846f0944be09b4bd82681195581","IPY_MODEL_8247a02b5c3f4334b2a1c4310a2f68cb"],"layout":"IPY_MODEL_9bf38641a02c4ada86284c10c0bbef59"}},"bbcdfb6a87cb4819a60b92e5c5a8bb0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0444c736069c4345b92bf65622b30c32","placeholder":"​","style":"IPY_MODEL_46af8a26937c4f81a9c660d3d059cb18","value":"Downloading (…)lve/main/config.json: 100%"}},"6f6a7846f0944be09b4bd82681195581":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d3fd482e01e41d7b6d827bff551f0ae","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cfffd2dd52284bf397a4d4536a269824","value":570}},"8247a02b5c3f4334b2a1c4310a2f68cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22b5eacef8f744838003478e789055a2","placeholder":"​","style":"IPY_MODEL_0458f623a1a14dcda1c015029cfdf695","value":" 570/570 [00:00&lt;00:00, 19.6kB/s]"}},"9bf38641a02c4ada86284c10c0bbef59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0444c736069c4345b92bf65622b30c32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46af8a26937c4f81a9c660d3d059cb18":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d3fd482e01e41d7b6d827bff551f0ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfffd2dd52284bf397a4d4536a269824":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"22b5eacef8f744838003478e789055a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0458f623a1a14dcda1c015029cfdf695":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14cc6cf67a0a4fa0b1c7f619247ec5e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_69738dd14d0e4a3f812862f00921526a","IPY_MODEL_22fd9077f8a847dbad5aa1ddacd27786","IPY_MODEL_bf71ef3677514349a87803421b4d6a8e"],"layout":"IPY_MODEL_80abf9582daa48f4a68edae53ccadc70"}},"69738dd14d0e4a3f812862f00921526a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a35d16ad237f49c781ac3d080bbf54cd","placeholder":"​","style":"IPY_MODEL_4d42e8bfc0fe48ae80ed8e9ab090f020","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"22fd9077f8a847dbad5aa1ddacd27786":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_992ebf7f05314f78b4a3dcef1f377d16","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_837a196db8684757a5809ddbd0d9de08","value":213450}},"bf71ef3677514349a87803421b4d6a8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82dc01326388413eb963747f64f69319","placeholder":"​","style":"IPY_MODEL_62ba0e3d9dee45a3af04d74743a4c11e","value":" 213k/213k [00:00&lt;00:00, 7.16MB/s]"}},"80abf9582daa48f4a68edae53ccadc70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a35d16ad237f49c781ac3d080bbf54cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d42e8bfc0fe48ae80ed8e9ab090f020":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"992ebf7f05314f78b4a3dcef1f377d16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"837a196db8684757a5809ddbd0d9de08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82dc01326388413eb963747f64f69319":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62ba0e3d9dee45a3af04d74743a4c11e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3b5f5eb98444ba98f7b6ddb85cc4af8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bea770415e37448089e75826350deef0","IPY_MODEL_b2d72849beb9470a8cd7505e977d5e41","IPY_MODEL_2c08eb82a5614e39b5339b2fcba09e00"],"layout":"IPY_MODEL_03af0567e54f4af9823df872a9c7207a"}},"bea770415e37448089e75826350deef0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77eebcc1d7134e2692dc98a514671afe","placeholder":"​","style":"IPY_MODEL_da0e2525e1e546759c8a7ebb00d401a5","value":"Downloading (…)/main/tokenizer.json: 100%"}},"b2d72849beb9470a8cd7505e977d5e41":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_03a3018f34904cc093606e270e511406","max":435797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c6d5a02c35234bd59f66cec9ce397ffe","value":435797}},"2c08eb82a5614e39b5339b2fcba09e00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3803df93d2574cfc915bd1b735a4d524","placeholder":"​","style":"IPY_MODEL_91c7057545f1440e8a0ec57e955afa0f","value":" 436k/436k [00:00&lt;00:00, 960kB/s]"}},"03af0567e54f4af9823df872a9c7207a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77eebcc1d7134e2692dc98a514671afe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da0e2525e1e546759c8a7ebb00d401a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03a3018f34904cc093606e270e511406":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6d5a02c35234bd59f66cec9ce397ffe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3803df93d2574cfc915bd1b735a4d524":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91c7057545f1440e8a0ec57e955afa0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"254adcb4707149b3b5fff31f8d923c84":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b400f0d6d6034b38951ba7a64dcd6cf3","IPY_MODEL_ba7d0fdc583a4b4983c91a797638752c","IPY_MODEL_e66a378f5b2c4600aca4d808cfca2fb8"],"layout":"IPY_MODEL_46d8d2927bbc438b9299f00fff511909"}},"b400f0d6d6034b38951ba7a64dcd6cf3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2b758e9a190400a806fa87d5043637f","placeholder":"​","style":"IPY_MODEL_44b4a146b0944e1a8a74eedd06bd2833","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"ba7d0fdc583a4b4983c91a797638752c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ae00f43d43640988c6b738ada52b272","max":435779157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1815e3cc0aad480294869ddbbbd7fb8a","value":435779157}},"e66a378f5b2c4600aca4d808cfca2fb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dd2ae8355f64a24aad7d27f709c6d4e","placeholder":"​","style":"IPY_MODEL_e5f9e6a2ff1747bc8ca3e536a99ef311","value":" 436M/436M [00:06&lt;00:00, 71.2MB/s]"}},"46d8d2927bbc438b9299f00fff511909":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2b758e9a190400a806fa87d5043637f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44b4a146b0944e1a8a74eedd06bd2833":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ae00f43d43640988c6b738ada52b272":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1815e3cc0aad480294869ddbbbd7fb8a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8dd2ae8355f64a24aad7d27f709c6d4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5f9e6a2ff1747bc8ca3e536a99ef311":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a1df9d4f3f0428b9906c1693850f9f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2ad8894fb435407e8ed3d5da42ba7deb","IPY_MODEL_2348f8de400d42c186af9119667af37a","IPY_MODEL_8a0b6b3d31a6496e9cf38f75f3889464"],"layout":"IPY_MODEL_58e0d63d3b3d4d57b097eb6c05e03a16"}},"2ad8894fb435407e8ed3d5da42ba7deb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55cd71bb62c84ea8b8aebe401954fa20","placeholder":"​","style":"IPY_MODEL_40002dece1344ecdb1221667c4d0d8ec","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"2348f8de400d42c186af9119667af37a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b496467e359f45cb91f5e23c090fb7d2","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75afd907000c4fa49150f84a66653907","value":231508}},"8a0b6b3d31a6496e9cf38f75f3889464":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22ded636316d415bb9c3451ed31d3590","placeholder":"​","style":"IPY_MODEL_c1b611bcfc1f49ef866f18b01fc49f00","value":" 232k/232k [00:00&lt;00:00, 556kB/s]"}},"58e0d63d3b3d4d57b097eb6c05e03a16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55cd71bb62c84ea8b8aebe401954fa20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40002dece1344ecdb1221667c4d0d8ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b496467e359f45cb91f5e23c090fb7d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75afd907000c4fa49150f84a66653907":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"22ded636316d415bb9c3451ed31d3590":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1b611bcfc1f49ef866f18b01fc49f00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73874600d27d47d49caa3cbecacd4010":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4fd27a3cfb894ffe9110435d2923e7e5","IPY_MODEL_da361491076146278f0e9673fd2e5808","IPY_MODEL_8cbc37bae1b1489db43729a5af78005c"],"layout":"IPY_MODEL_28e18da587ca4476ae2775df921edf2f"}},"4fd27a3cfb894ffe9110435d2923e7e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c45b96c5790a4bd08e26634d34b00e02","placeholder":"​","style":"IPY_MODEL_c159b26298c04ee39787e9aaea19c9b7","value":"Downloading (…)okenizer_config.json: 100%"}},"da361491076146278f0e9673fd2e5808":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e63ea082f54476e8d0d202e3a328b6c","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aed796e68e1b4405b93496f870a508e5","value":28}},"8cbc37bae1b1489db43729a5af78005c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d707f5b2514d452cb642521079dd8ecb","placeholder":"​","style":"IPY_MODEL_e6bbfc5a675444acb0b832da5fc63408","value":" 28.0/28.0 [00:00&lt;00:00, 1.15kB/s]"}},"28e18da587ca4476ae2775df921edf2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c45b96c5790a4bd08e26634d34b00e02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c159b26298c04ee39787e9aaea19c9b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e63ea082f54476e8d0d202e3a328b6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aed796e68e1b4405b93496f870a508e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d707f5b2514d452cb642521079dd8ecb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6bbfc5a675444acb0b832da5fc63408":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"642526fbd4d64eb298c9963dd07173c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42678b05e089459b93b05d05c1dcfa92","IPY_MODEL_62ecbcd2cf6f4742a1c45608b8030cf4","IPY_MODEL_7336d3a9622f486db40ab9c91107b4b6"],"layout":"IPY_MODEL_6ae67d0f0ddf4b37aadd56145857ce74"}},"42678b05e089459b93b05d05c1dcfa92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc65c6e1803a4240bc0118f6077ec321","placeholder":"​","style":"IPY_MODEL_0d33de5bed534625b404f109256a493a","value":"Downloading (…)lve/main/config.json: 100%"}},"62ecbcd2cf6f4742a1c45608b8030cf4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8badace30e2431885dead135ce97c3c","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_590820d5ac124e5b9c79da8b459e9e6e","value":483}},"7336d3a9622f486db40ab9c91107b4b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d41313cebcc0486abab86bb3d321e02f","placeholder":"​","style":"IPY_MODEL_98ade5be0fca4d6ebbde6e743b19fb83","value":" 483/483 [00:00&lt;00:00, 21.3kB/s]"}},"6ae67d0f0ddf4b37aadd56145857ce74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc65c6e1803a4240bc0118f6077ec321":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d33de5bed534625b404f109256a493a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8badace30e2431885dead135ce97c3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"590820d5ac124e5b9c79da8b459e9e6e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d41313cebcc0486abab86bb3d321e02f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98ade5be0fca4d6ebbde6e743b19fb83":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b6211eb7cf84ae5bcae10ab9c52c6c2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_37eb2764460b483da8490a576da76aaf","IPY_MODEL_b556def56f8646e2a9d06c25205bd815","IPY_MODEL_fa8173b019dd4eecab6fccf513946165"],"layout":"IPY_MODEL_7971804787e143a68588b01df694b8db"}},"37eb2764460b483da8490a576da76aaf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2cd64e8ad0847b890dd425d3e5d13bb","placeholder":"​","style":"IPY_MODEL_2fe28310e7ba40609f9292c30528c9a1","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"b556def56f8646e2a9d06c25205bd815":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_969c23c4fe854cb6ae57fe36b364ac51","max":267967963,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a074ddca842b4f3c8ae5001d019b793e","value":267967963}},"fa8173b019dd4eecab6fccf513946165":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_884645a3b2e3458eb770794c7cf1d6f9","placeholder":"​","style":"IPY_MODEL_8040403ccec94434beae96b4299b9b0d","value":" 268M/268M [00:03&lt;00:00, 89.4MB/s]"}},"7971804787e143a68588b01df694b8db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2cd64e8ad0847b890dd425d3e5d13bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fe28310e7ba40609f9292c30528c9a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"969c23c4fe854cb6ae57fe36b364ac51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a074ddca842b4f3c8ae5001d019b793e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"884645a3b2e3458eb770794c7cf1d6f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8040403ccec94434beae96b4299b9b0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
